# WARNING: This will irreversibly destroy all data on the specified disk(s) on all nodes.
# Use this to wipe disks on all nodes in a rook-ceph cluster.
# 1. Adjust the DISK environment variable to specify the correct disk to wipe.
# 2. k apply -f deploy/config/wipe.yaml
# 3. Wait for all daemonset pods to complete.
# 4. k delete -f deploy/config/wipe.yaml
# For each node, reboot.
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: disk-wipe
  labels:
    app.kubernetes.io/name: disk-wipe
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: disk-wipe
  template:
    metadata:
      labels:
        app.kubernetes.io/name: disk-wipe
    spec:
      # DaemonSets ignore Never; default is Always. Pod will run once and exit.
      hostPID: true
      tolerations:
        - operator: Exists
      volumes:
        - name: rook-data-dir
          hostPath:
            path: /var/lib/rook
      containers:
        - name: disk-wipe
          image: alpine:3.20
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          volumeMounts:
            - name: rook-data-dir
              mountPath: /node/rook-data
          env:
            - name: DISK
              value: "/dev/sda"
          command:
            - /bin/sh
            - -c
            - |
              set -eu
              rm -rf /node/rook-data/* || true
              apk add --no-cache gptfdisk util-linux parted sgdisk >/dev/null
              sgdisk --zap-all "$DISK"
              dd if=/dev/zero of="$DISK" bs=1M count=100 oflag=direct conv=fsync
              blkdiscard "$DISK" || true
              partprobe "$DISK"
          resources:
            limits:
              cpu: 4
              memory: "128Mi"
            requests:
              cpu: "100m"
              memory: "128Mi"
